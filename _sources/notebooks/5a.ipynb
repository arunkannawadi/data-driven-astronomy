{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Photometric redshifts - Introduction\n",
    "\n",
    "In this activity, we're going to use decision trees to determine the redshifts of galaxies from their photometric colours.\n",
    "We'll use galaxies where accurate spectroscopic redshifts have been calculated as our gold standard.\n",
    "We will learn how to assess the accuracy of the decision trees predictions and have a look at validation of our model.\n",
    "\n",
    "We will also have a quick look at how this problem might be approached without using machine learning.\n",
    "This will highlight some of the limitations of the classical approach and demonstrate why a machine learning approach is ideal for this type of problem.\n",
    "\n",
    "This activity is based on the scikit-learn example on [Photometric Redshifts of Galaxies](http://ogrisel.github.io/scikit-learn.org/sklearn-tutorial/tutorial/astronomy/regression.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Magnitudes and colours\n",
    "\n",
    "We will be using flux magnitudes from the Sloan Digital Sky Survey (SDSS) catalogue to create colour indices.\n",
    "Flux magnitudes are the total flux (or light) received in five frequency bands (*u*, *g*, *r*, *i* and *z*).\n",
    "\n",
    "![SDSS filter response](https://groklearning-cdn.com/modules/TYPnbyWbJVHGcaNudF4Nak/plot_sdss_filters_11.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Colours correspond to the wavelengths of the light we observe. Longer wavelengths appear red and shorter wavelengths appear blue or violet. Similarly, the astronomical colour (or colour index) is the difference between the magnitudes of two filters, i.e. *u - g* or *i - z*. You can read more about the filters and how the cameras work at http://voyages.sdss.org/preflight/light/filters/, https://www.sdss.org/instruments/camera/, and http://voyages.sdss.org/.\n",
    "\n",
    "This index is one way to characterise the colours of galaxies. For example, if the *u - g* index is high then the object is brighter in ultra violet frequencies than it is in visible green frequencies.\n",
    "\n",
    "Colour indices act as an approximation for the spectrum of the object and are useful for classifying stars into different types."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What data do we need?\n",
    "\n",
    "To calculate the redshift of a distant galaxy, the most accurate method is to observe the optical emission lines and measure the shift in wavelength.\n",
    "However, this process can be time consuming and is thus infeasible for large samples.\n",
    "\n",
    "For many galaxies we simply don't have spectroscopic observations.\n",
    "\n",
    "Instead, we can calculate the redshift by measuring the flux using a number of different filters and comparing this to models of what we expect galaxies to look like at different redshifts.\n",
    "\n",
    "In this activity, we will use machine learning to obtain photometric redshifts for a large sample of galaxies.\n",
    "We will use the colour indices (*u - g*, *g - r*, *r - i* and *i - z*) as our input and a subset of sources with spectroscopic redshifts as the training dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision tree algorithms\n",
    "\n",
    "Decision trees are a tool that can be used for both classification and regression.\n",
    "In this module we will look at regression, however, in the next module we will see how they can be used as classifiers.\n",
    "\n",
    "Decision trees map a set of input features to their corresponding output targets.\n",
    "This is done through a series of individual decisions where each decision represents a node (or branching) of the tree.\n",
    "\n",
    "The following figure shows the decision tree our proverbial robot tennis player Robi used in the lectures to try and decide whether to play tennis on a particular day.\n",
    "\n",
    "![A simple decision tree on whether or not to play tennis](https://groklearning-cdn.com/modules/oqviMCYkoRqeu76VKnwRsn/tennis.png)\n",
    "\n",
    "Each node represents a decision that the robot needs to make (or assess) to reach a final decision.\n",
    "In this example, the decision tree will be passed a set of input features (Outlook, Humidity and Wind) and will return an output of whether to play or not.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision tree for regression\n",
    "\n",
    "In decision trees for real-world tasks, each decision is typically more complex, involving measured values, not just categories.\n",
    "\n",
    "Instead of the input values for humidity being `Normal` or `High` and wind being `Strong` or `Weak` we might see a percentage between 0 and 100 for humidity and a wind speed in km/hr for wind.\n",
    "Our decisions might then be humidity < 40% or wind < 5 km/hr.\n",
    "\n",
    "The output of regression is a real number.\n",
    "So, instead of the two outputs of `Play` and `Don't Play` we have a probability of whether we will play that day.\n",
    "\n",
    "The decision at each branch is determined from the training data by the decision tree learning algorithm.\n",
    "Each algorithm employs a different metric (e.g. Gini impurity or information gain) to find the decision that splits the data most effectively.\n",
    "\n",
    "For now, just need to know that a decision tree is a series of decisions, each made on a single feature of the data.\n",
    "The end point of all the branches is a set of desired target values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decision trees in Python\n",
    "\n",
    "The inputs to our decision tree are the colour indices from photometric imaging and our output is a photometric redshift.\n",
    "Our training data uses accurate spectroscopic measurements.\n",
    "\n",
    "The decision tree will look something like the following.\n",
    "\n",
    "![A decision tree for regression](https://groklearning-cdn.com/modules/5PUcnT5fqRn5wTUXw7kg2B/decisiontree_1.png)\n",
    "\n",
    "We can see how our calculated colour indices are input as features at the top and through a series of decision nodes a target redshift value is reached and output.\n",
    "\n",
    "We will be using the Python machine learning library [scikit-learn](http://scikit-learn.org/stable) which provides several machine learning algorithms.\n",
    "\n",
    "The [scikit-learn decision tree regression](http://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression.html) takes a set of input features and the corresponding target values, and constructs a decision tree model that can be applied to new data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sloan Digital Sky Survey data\n",
    "\n",
    "We have provided the Sloan data in NumPy binary format (**.npy**) in a file called **sdss_galaxy_colors.npy**.\n",
    "The Sloan data is stored in a [NumPy structured array](https://docs.scipy.org/doc/numpy/user/basics.rec.html) and looks like this:\n",
    "\n",
    "```\n",
    "u\tg\tr\ti\tz\t...\tredshift\n",
    "19.84\t19.53\t19.47\t19.18\t19.11\t...\t0.54\n",
    "19.86\t18.66\t17.84\t17.39\t17.14\t...\t0.16\n",
    "...\t...\t...\t...\t...\t...\t...\n",
    "18.00\t17.81\t17.77\t17.73\t17.73\t...\t0.47\n",
    "```\n",
    "\n",
    "It also include **spec_class** and **redshift_err** columns we don't need in this activity.\n",
    "The data can be loaded using:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "data = np.load('sdss_galaxy_colors.npy')\n",
    "print(data[0])\n",
    "```\n",
    "\n",
    "The ```data[0]``` corresponds to the first row of the table above.\n",
    "Individual named columns can be accessed like this:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "data = np.load('sdss_galaxy_colors.npy')\n",
    "print(data['u'])\n",
    "```\n",
    "\n",
    "Each flux magnitude column can be accessed in the data array as `data['u']`, `data['g']` etc.\n",
    "The redshifts can accessed with `data['redshift']`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Features and targets\n",
    "\n",
    "Write a `get_features_targets` function that splits the training data into input features and their corresponding targets.\n",
    "In our case, the inputs are the 4 colour indices and our targets are the corresponding redshifts.\n",
    "\n",
    "Your function should return a tuple of:\n",
    "\n",
    "- **features**: a NumPy array of dimensions *m* â¨‰ 4, where *m* is the number of galaxies;\n",
    "- **targets**: a 1D NumPy array of length *m*, containing the redshift for each galaxy.\n",
    "\n",
    "The data argument will be the structured array described on the previous slide.\n",
    "The *u* flux magnitudes and redshifts can be accessed as a column with `data['u']` and `data['redshift']`.\n",
    "\n",
    "The four features are the colours *u - g*, *g - r*, *r - i* and *i - z*.\n",
    "To calculate the first column of features, subtract the *u* and *g* columns, like this:\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "data = np.load('sdss_galaxy_colors.npy')\n",
    "print(data['u'] - data['g'])\n",
    "```\n",
    "\n",
    "The features for the first 2 galaxies in the example data should be:\n",
    "\n",
    "```python\n",
    "[[ 0.31476  0.0571   0.28991  0.07192]\n",
    " [ 1.2002   0.82026  0.45294  0.24665]]\n",
    "```\n",
    "\n",
    "And the first 2 targets should be:\n",
    "\n",
    "```python\n",
    "[ 0.539301   0.1645703]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::::::{admonition} Hint: set up your **features** array with zeros\n",
    ":class: tip\n",
    "You can set up the features array with zeros and then set each column to the corresponding calculated feature.\n",
    "\n",
    "```python\n",
    "features = np.zeros((data.shape[0], 4))\n",
    "features[:, 0] = data['u'] - data['g']\n",
    "```\n",
    "::::::\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_features_targets(data):\n",
    "  targets = data['redshift']\n",
    "  features = np.array([data['u'] - data['g'],\n",
    "                       data['g'] - data['r'],\n",
    "                       data['r'] - data['i'],\n",
    "                       data['i'] - data['z']\n",
    "                      ])\n",
    "  return features.T, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.31476 0.0571  0.28991 0.07192]\n",
      " [1.2002  0.82026 0.45294 0.24665]]\n",
      "[0.539301  0.1645703]\n"
     ]
    }
   ],
   "source": [
    "# load the data\n",
    "data = np.load('data/sdss_galaxy_colors.npy')\n",
    "\n",
    "# call our function\n",
    "features, targets = get_features_targets(data)\n",
    "\n",
    "# print the shape of the returned arrays\n",
    "print(features[:2])\n",
    "print(targets[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: Decision tree regressor\n",
    "\n",
    "We are now going to use our features and targets to train a decision tree and then make a prediction. We are going to use the [DecisionTreeRegressor](http://scikit-learn.org/stable/auto_examples/tree/plot_tree_regression.html) class from the `sklearn.tree` module.\n",
    "\n",
    "The decision tree regression learning algorithm is initialised with:\n",
    "\n",
    "```python\n",
    "dtr = DecisionTreeRegressor()\n",
    "```\n",
    "\n",
    "We will discuss some optimisations later in the activity, but for now we are just going to use the default values.\n",
    "\n",
    "To train the model, we use the `fit` method with the `features` and `targets` we created earlier:\n",
    "\n",
    "```python\n",
    "dtr.fit(features, targets)\n",
    "```\n",
    "\n",
    "The decision tree is now trained and ready to make a prediction:\n",
    "\n",
    "```python\n",
    "predictions = dtr.predict(features)\n",
    "```\n",
    "\n",
    "`predictions` is an array of predicted values corresponding to each of the input variables in the array.\n",
    "\n",
    "Your task is to put this together for our photometric redshift data.\n",
    "Use the comments to guide your implementation.\n",
    "\n",
    "Finally, print the first 4 predictions. It should print this:\n",
    "\n",
    "```python\n",
    "[ 0.539301    0.1645703   0.04190006  0.04427702]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.539301   0.1645703  0.04190006 0.04427702]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "\n",
    "# load the data and generate the features and targets\n",
    "data = np.load('data/sdss_galaxy_colors.npy')\n",
    "features, targets = get_features_targets(data)\n",
    "\n",
    "# initialize model\n",
    "dtr = DecisionTreeRegressor()\n",
    "\n",
    "# train the model\n",
    "dtr.fit(features, targets)\n",
    "\n",
    "# make predictions using the same features\n",
    "predictions = dtr.predict(features)\n",
    "\n",
    "# print out the first 4 predicted redshifts\n",
    "print(predictions[:4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating our results: accuracy\n",
    "\n",
    "So we trained a decision tree! Great...but how do we know if the tree is actually any good at predicting redshifts?\n",
    "\n",
    "In regression we compare the predictions generated by our model with the actual values to test how well our model is performing. The difference between the predicted values and actual values (sometimes referred to as residuals) can tell us a lot about where our model is performing well and where it is not.\n",
    "\n",
    "While there are a few different ways to characterise these differences, in this tutorial we will use the median of the differences between our predicted and actual values. This is given by:\n",
    "\n",
    "$$ med\\_diff = median(|Y_\\text{i, pred} - Y_\\text{i,act}|) $$\n",
    "\n",
    "Where  $|  |$ denotes the absolute value of the difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Calculating the median difference\n",
    "\n",
    "In this problem we will implement the function median_diff. The function should calculate the median residual error of our model, i.e. the median difference between our predicted and actual redshifts.\n",
    "\n",
    "The median_diff function takes two arguments â€“ the predicted and actual/target values.\n",
    "When we use this function later in the tutorial, these will corresponding to the predicted redshifts from our decision tree and their corresponding measured/target values.\n",
    "\n",
    "The median of differences should be calculated according to the formula above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# write a function that calculates the median of the differences\n",
    "# between our predicted and actual values\n",
    "def median_diff(predicted, actual):\n",
    "  return np.nanmedian(np.abs(predicted-actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load testing data\n",
    "targets = np.load('data/targets.npy')  # currently missing\n",
    "predictions = np.load('data/predictions.npy')  # currently missing\n",
    "\n",
    "# call your function to measure the accuracy of the predictions\n",
    "diff = median_diff(predictions, targets)\n",
    "\n",
    "# print the median difference\n",
    "print(\"Median difference: {:0.3f}\".format(diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating our results: Validation\n",
    "\n",
    "We previously used the same data for training and testing our decision trees.\n",
    "\n",
    "This gives an unrealistic estimate of how accurate the model will be on previously unseen galaxies because the model has been optimised to get the best results on the training data.\n",
    "\n",
    "The simplest way to solve this problem is to split our data into* training* and *testing* subsets:\n",
    "\n",
    "```python\n",
    "# initialise and train the decision tree\n",
    "dtr = DecisionTreeRegressor()\n",
    "dtr.fit(train_features, train_targets)\n",
    "\n",
    "# get a set of prediction from the test input features\n",
    "predictions = dtr.predict(test_features)\n",
    "â€‹\n",
    "# compare the accuracy of the pediction againt the actual values\n",
    "print(calculate_rmsd(predictions, test_targets))\n",
    "```\n",
    "\n",
    "This method of validation is the most basic approach to validation and is called held-out validation.\n",
    "We will use the $med\\_diff$ accuracy measure and hold-out validation in the next problem to assess the accuracy of our decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Validating our model\n",
    "\n",
    "In this problem, we will use `median_diff` from the previous question to validate the decision tree model.\n",
    "Your task is to complete the `validate_model` function.\n",
    "\n",
    "The function should split the features and targets into train and test subsets, like this 50:50 split for features:\n",
    "\n",
    "```python\n",
    "split = features.shape[0]//2\n",
    "train_features = features[:split]\n",
    "test_features = features[split:]\n",
    "```\n",
    "\n",
    "Your function should then use the training split (`train_features` and `train_targets`) to train the model.\n",
    "\n",
    "Finally, it should measure the accuracy of the model using `median_diff` on the `test_targets` and the predicted redshifts from `test_features`.\n",
    "\n",
    "The function should take 3 arguments:\n",
    "\n",
    "- model: the decision tree regressor;\n",
    "- features - the features for the data set;\n",
    "- targets - The targets for the data set.\n",
    "\n",
    "::::::{admonition} Hint: keep features and targets together!\n",
    ":class: tip\n",
    "When splitting the `features` and `targets` be careful to ensure that the `train_features` have the correct `train_targets`, i.e. `train_features[0]` corresponds to `train_targets[0]` etc.\n",
    "::::::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring the output tree\n",
    "\n",
    "But what does the decision tree actually look like?\n",
    "\n",
    "![Decision tree visualization](https://groklearning-cdn.com/modules/E3u6xHw8LC8V75vTJ5ZvdG/decision_tree.jpg)\n",
    "\n",
    "You can see how the decision is made at each node as well as the number of samples which reach that node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {"tags":["hide-input"]},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pydotplus'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/arunkannawadi/Desktop/code/data-driven-astronomy/data_driven_astronomy/5.ipynb Cell 23\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/arunkannawadi/Desktop/code/data-driven-astronomy/data_driven_astronomy/5.ipynb#ch0000027?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/arunkannawadi/Desktop/code/data-driven-astronomy/data_driven_astronomy/5.ipynb#ch0000027?line=1'>2</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpydotplus\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpydotplus\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/arunkannawadi/Desktop/code/data-driven-astronomy/data_driven_astronomy/5.ipynb#ch0000027?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtree\u001b[39;00m \u001b[39mimport\u001b[39;00m DecisionTreeRegressor,export_graphviz\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/arunkannawadi/Desktop/code/data-driven-astronomy/data_driven_astronomy/5.ipynb#ch0000027?line=4'>5</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_features_targets\u001b[39m(data):\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pydotplus'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pydotplus as pydotplus\n",
    "from sklearn.tree import DecisionTreeRegressor, export_graphviz\n",
    "\n",
    "\n",
    "data = np.load('data/sdss_galaxy_colors.npy')\n",
    "features, targets = get_features_targets(data)\n",
    "\n",
    "# Initialize model\n",
    "dtr = DecisionTreeRegressor(max_depth=3)    # We will come to this input in the next tutorial\n",
    "\n",
    "# Split the data into training and testing\n",
    "split_index = int(0.5 * len(features))\n",
    "train_features = features[:split_index]\n",
    "train_targets = targets[:split_index]\n",
    "\n",
    "dtr.fit(train_features, train_targets)\n",
    "\n",
    "dot_data = export_graphviz(dtr, out_file=None,feature_names=['u - g', 'g - r', 'r - i', 'i - z'])\n",
    "graph = pydotplus.graph_from_dot_data(dot_data)\n",
    "graph.write_jpg(\"output/decision_tree.jpg\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The median of differences of $\\approx 0.02$.\n",
    "This means that half of our galaxies have a error in the prediction of $\\lt 0.02$, which is pretty good.\n",
    "One of the reason we chose the median of differences as our accuracy measure is that it gives a fair representation of the errors especially when the distribution of errors is skewed.\n",
    "The graph below shows the distribution of residuals (differences) for our model along with the median and interquartile values.\n",
    "\n",
    "![Residual distribution](https://groklearning-cdn.com/modules/7sF2Vds9J4hmG39GYztEPD/residuals.png)\n",
    "\n",
    "As you can tell the distribution is very skewed. We have zoomed in here, but the tail of the distribution goes all the way out to 6.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The effect of training set size\n",
    "\n",
    "The number of galaxies we use to train the model has a big impact on how accurate our predictions will be.\n",
    "This is the same with most machine learning methods: the more data that they are trained with, the more accurate their prediction will be.\n",
    "\n",
    "Here is how our median difference changes with training set size:\n",
    "\n",
    "```\n",
    "Training galaxies\tmedian_diff\n",
    "50\t0.048\n",
    "500\t0.026\n",
    "5000\t0.023\n",
    "50000\t0.022\n",
    "```\n",
    "\n",
    "Understanding how the accuracy of the model changes with sample size is important to understanding the limitations of our model.\n",
    "We are approaching the accuracy limit of the decision tree model (for our redshift problem) with a training sample size of 25,000 galaxies.\n",
    "\n",
    "The only way we could further improve our model would be to use more features.\n",
    "This might include more colour indices or even the errors associated with the measured flux magnitudes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Before machine learning\n",
    "\n",
    "Before machine learning, we would have tried to solve this problem with regression â€” by constructing an empirical model to predict how the dependent variable (redshift) varies with one or more independent variables (the colour indices).\n",
    "\n",
    "A plot of how the colours change with redshift tells us that it is quite a complex function, for example redshift versus *u - g*:\n",
    "\n",
    "![Redshift vs. u-g colour](https://groklearning-cdn.com/problems/SKULeCQhsJ4Ew7DtbPd6ri/redshift-U-G.png)\n",
    "\n",
    "One approach would be to construct a multi-variate non-linear regression model.\n",
    "Perhaps using a least squares fitting to try and determine the best fit parameters.\n",
    "The model would be quite complex; based on the above plot, a dampened inverse sine function would be a good starting point for such a model.\n",
    "\n",
    "While we could try such an approach the function would be overly complex and there is no guarantee that it would yield very promising results.\n",
    "Another approach would be to plot a colour-index vs colour-index plot using an additional colour scale to show redshift.\n",
    "The following plot is an example of such a plot.\n",
    "\n",
    "![Colour colour redshift plot](https://groklearning-cdn.com/problems/PEBuctPTbbMiee3vv9tkL3/redshift-colour-colour.png)\n",
    "\n",
    "It shows that we get reasonably well defined regions where redshifts are similar.\n",
    "If we were to make a contour map of the redshifts in the colour index vs colour index space we would be able to get an estimate of the redshift for new data points based on a combination of their colour indices.\n",
    "This would lead to redshift estimates with significant uncertainties attached to them.\n",
    "\n",
    "In the next problem you will re-create the Colour-index vs Colour-index plot with redshift as colour bar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Colour Colour Redshift plot\n",
    "\n",
    "Your task here is to simply recreate the plot above.\n",
    "\n",
    "You should use the `pyplot` module of `matplotlib` which has already been imported and can be accessed through `plt`.\n",
    "In particular you can use the `plt.scatter()` function, with additional arguments `s`, `c` and `cmap`.\n",
    "\n",
    "We are interested in the *u - g* and *r - i* colour indices.\n",
    "\n",
    "You can make use of the `plt.colorbar()` function to show your scatter plots colour argument(`c`) to a colour bar on the side of the plot.\n",
    "\n",
    "Make sure you implement `x` and `y` labels and give your plot a title.\n",
    "\n",
    "::::::{admonition} Gotchas\n",
    ":class: tip\n",
    "In order to get your plot working with colours you will need to set the optional parameter `lw=number`.\n",
    "There are a lot of small points and unless we set the linewidth to zero the line surrounding each point will block out the point itself.\n",
    "\n",
    "You should also specify the size of your plot points with the additional call arguments `s=number`.\n",
    "A reasonable place to start might be 1, but see what it looks like.\n",
    "::::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "\n",
    "# Complete the following to make the plot\n",
    "data = np.load('data/sdss_galaxy_colors.npy')\n",
    "\n",
    "# Get a colour map\n",
    "cmap = plt.get_cmap('YlOrRd')\n",
    "\n",
    "# Define our colour indexes u-g and r-i\n",
    "x = data['u'] - data['g']\n",
    "y = data['r'] - data['i']\n",
    "# Make a redshift array\n",
    "z = data['redshift']\n",
    "\n",
    "# Create the plot with plt.scatter and plt.colorbar\n",
    "number = 0.8\n",
    "plt.scatter(x,y,c=z,s=number, cmap=cmap, lw=number)\n",
    "\n",
    "# Define your axis labels and plot title\n",
    "plt.xlabel('u-g')\n",
    "plt.ylabel('r-i')\n",
    "# Set any axis limits\n",
    "plt.xlim([-0.5, 2.5])\n",
    "plt.ylim([-0.49, 1.0])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this activity, we implemented some decision tree models to help predict the redshift of galaxies based on their measured colour indices.\n",
    "We learnt that there are several ways to assess the accuracy of the model and in our validations we used the median of the residuals.\n",
    "\n",
    "We touched on how the problem might be approached without machine learning and found that there isn't too much available that can be simply used.\n",
    "\n",
    "We also learnt why we need to cross validate the model and that this is done by splitting the data into seperate training and testing subsets.\n",
    "We will look at k-fold cross validation in the next tutorial; where the testing and training are changed to include various combinations of $k$ seperate subsets.\n",
    "\n",
    "In the next tutorial we will also explore how decision trees have a tendency to overfit the data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-driven-astronomy",
   "language": "python",
   "name": "data-driven-astronomy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
