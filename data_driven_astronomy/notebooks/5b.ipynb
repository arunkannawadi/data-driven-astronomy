{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving and evaluating our classifier\n",
    "\n",
    "In start this activity by looking at how decision trees tend to overfit the data if they are left unchecked. Over fitting the data means they try to account for the outlying data points at the cost of the prediction accuracy of the general trend.\n",
    "\n",
    "We will also look at k-fold cross validation. This is a more robust method of validation than the held-out method we used previously.\n",
    "\n",
    "In k-fold cross validation, we can test every example once. This is done by splitting the data set into $k$ subsets and training/testing the model $k$ times using different combinations of the subsets.\n",
    "\n",
    "Finally, we look at how accurate our model is on QSOs compared with other galaxies. As mentioned in the lectures, QSOs are galaxies that have an Active Galactic Nucleus (AGN). The AGN makes the galaxy brighter and as such they are detectable with the SDSS instruments out to much higher redshifts.\n",
    "\n",
    "We will use the same data set as the first activity and even some of functions we wrote in previous questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees have many advantages: they are simple to implement, easy to interpret, the data doesn't require too much preparation, and they are reasonably efficient computationally.\n",
    "\n",
    "Decision trees do have some limitations though, one of the biggest being they tend to over fit the data. What this means is that if they are left unchecked they will create an overly complicated tree that attempts to account for outliers in the data. This comes at the expense of the accuracy of the general trend.\n",
    "\n",
    "Part of the reason for this over-fitting is that the algorithm works by trying to optimise the decision locally at each node. There are ways in which this can be mitigated and in the next problem we will see how constraining the number of decision node rows (the tree depth) impacts on the accuracy of our predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to see how the tree is overfitting we would like to examine how our decision tree performs for different tree depths. Specifically, we would like to see how it performs on test data compared to the data that was used to train it.\n",
    "\n",
    "Na√Øvely we'd expect, the deeper the tree, the better it should perform. However, as the model overfits we see a difference in its accuracy on the training data and the more general testing data.\n",
    "\n",
    "We can control the depth of decision tree learned, using an argument to `DecisionTreeRegressor`. For example, to set the maximum depth to 5:\n",
    "\n",
    "```python\n",
    "dtr = DecisionTreeRegressor(max_depth=5)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the function `accuracy_by_treedepth`. The function should return the median difference for both the testing and training data sets for each of the tree depths in depths.\n",
    "\n",
    "`accuracy_by_treedepth` should take the following arguments:\n",
    "\n",
    "- features and targets (as in previous problems);\n",
    "- depths: an array of tree depths to be used as the max_depth of the decision tree regressor.\n",
    "\n",
    "Your function should return two lists (or arrays) containing the median_diff values for the predictions made on the training and test sets using the maximum tree depths given by the depths.\n",
    "\n",
    "For example, if `depths` is `[3, 5, 7]`, then your function should return two lists of length 3. You can choose the size of the split between your testing and training data (if in doubt, 50:50 is fine).\n",
    "\n",
    "We've included code to plot the differences as a function of tree depths. You should take a moment to familiarise yourself with what each line is doing. If your code is working well then your plot should look a bit like the following:\n",
    "\n",
    "![Median differences as a function of tree depth for training and testing datasets](https://groklearning-cdn.com/problems/8Cet6iLGMbP2L8t7SVkEEg/overfitting.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run 5a.ipynb\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Complete the following function\n",
    "def accuracy_by_treedepth(features, targets, depths):\n",
    "  # split the data into testing and training sets\n",
    "\n",
    "  # Initialise arrays or lists to store the accuracies for the below loop\n",
    "\n",
    "  # Loop through depths\n",
    "  for depth in depths:\n",
    "    # initialize model with the maximum depth.\n",
    "    dtr = DecisionTreeRegressor(max_depth=depth)\n",
    "\n",
    "    # train the model using the training set\n",
    "\n",
    "    # Get the predictions for the training set and calculate their med_diff\n",
    "\n",
    "    # Get the predictions for the testing set and calculate their med_diff\n",
    "\n",
    "  # Return the accuracies for the training and testing sets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "  data = np.load('data/sdss_galaxy_colors.npy')\n",
    "  features, targets = get_features_targets(data)\n",
    "\n",
    "  # Generate several depths to test\n",
    "  tree_depths = [i for i in range(1, 36, 2)]\n",
    "\n",
    "  # Call the function\n",
    "  train_med_diffs, test_med_diffs = accuracy_by_treedepth(features, targets, tree_depths)\n",
    "  print(\"Depth with lowest median difference : {}\".format(tree_depths[test_med_diffs.index(min(test_med_diffs))]))\n",
    "\n",
    "  # Plot the results\n",
    "  train_plot = plt.plot(tree_depths, train_med_diffs, label='Training set')\n",
    "  test_plot = plt.plot(tree_depths, test_med_diffs, label='Validation set')\n",
    "  plt.xlabel(\"Maximum Tree Depth\")\n",
    "  plt.ylabel(\"Median of Differences\")\n",
    "  plt.legend()\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::::::{dropdown} Click here for a solution\n",
    "\n",
    "Here we use indices to access the data in our features and targets, however it would have been equally valid to create 4 explicit arrays `train_features`, `test_features`, `train_targets` and `test_targets`,\n",
    "\n",
    "We construct two lists `accuracies_train` and `accuracies_test` to which we can append the *med_diff* values for each depth in the for loop.\n",
    "\n",
    "The decision tree is instantiated upon each iteration of the loop with a new `max_depth` which is taken from the argument in the *for* loop. We also set the `random_seed=0`. This relates to the pseudo random numbers used to generate the tree and by setting it equal to 0 we are ensuring that all depths are tested in a consistent manner. This is not a requirement of your solution, but it is certainly recommended.\n",
    "\n",
    "We then train the tree and get predictions for the training and testing features. As previously mentioned, it is generally bad to use the same training data to assess the accuracy of the model, however as mentioned in the problems context we are using it for comparison.\n",
    "\n",
    "Finally we use the predictions for each set to calculate the *med_diff* (using our previously written function) and append it to our `accuracies_*` from earlier.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Complete the following function\n",
    "def accuracy_by_treedepth(features, targets, depths):\n",
    "  # split the data into testing and training sets\n",
    "  split = features.shape[0]//2\n",
    "  train_features, test_features = features[:split], features[split:]\n",
    "  train_targets, test_targets = targets[:split], targets[split:]\n",
    "\n",
    "  # Initialise arrays or lists to store the accuracies for the below loop\n",
    "  train_diffs = []\n",
    "  test_diffs = []\n",
    "\n",
    "  # Loop through depths\n",
    "  for depth in depths:\n",
    "    # initialize model with the maximum depth. \n",
    "    dtr = DecisionTreeRegressor(max_depth=depth)\n",
    "\n",
    "    # train the model using the training set\n",
    "    dtr.fit(train_features, train_targets)\n",
    "\n",
    "    # Get the predictions for the training set and calculate their med_diff\n",
    "    predictions = dtr.predict(train_features)\n",
    "    train_diffs.append(median_diff(train_targets, predictions))\n",
    "\n",
    "    # Get the predictions for the testing set and calculate their med_diff\n",
    "    predictions = dtr.predict(test_features)\n",
    "    test_diffs.append(median_diff(test_targets, predictions))\n",
    "\n",
    "  # Return the accuracies for the training and testing sets\n",
    "  return train_diffs, test_diffs\n",
    "```\n",
    "\n",
    "::::::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discussion of results\n",
    "\n",
    "We can see that the accuracy of the decision tree on the *training set* gets better as we allow the tree to grow to greater depths. In fact, at a depth of 27 our errors goes to zero!\n",
    "\n",
    "Conversly, the accuracy measure of the predictions for the test set gets better initially and then worse at larger tree depths. At a tree depth ~19 the decision tree starts to overfit the data. This means it tries to take into account outliers in the training set and loses its general predictive accuracy.\n",
    "\n",
    "Overfitting is a common problem with decision trees and can be circumvented by adjusting parameters like the tree depth or setting a minimum number of cases at each node. For now, we will set a maximum tree depth of 19 to prevent over-fitting in our redshift problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation\n",
    "\n",
    "The method we used to validate our model so far is known as hold-out validation.\n",
    "Hold out validation splits the data in two, one set to test with and the other to train with.\n",
    "Hold out validation is the most basic form of validation.\n",
    "\n",
    "While hold-out validation is better than no validation, the measured accuracy (i.e. our median of differences) will vary depending on how we split the data into testing and training subsets.\n",
    "The *med_diff* that we get from one randomly sampled training set will vary to that of a different random training set of the same size.\n",
    "\n",
    "In order to be more certain of our models accuracy we should use $k$-fold cross validation.\n",
    "$k$-fold validation works in a similar way to hold-out except that we split the data into $k$ subsets.\n",
    "We train and test the model $k$ times, recording the accuracy each time.\n",
    "Each time we use a different combination of $k-1$ subsets to train the model and the final $k^\\text{th}$ subset to test.\n",
    "We take the average of the $k$ accuracy measurements to be the overall accuracy of the the model.\n",
    "\n",
    "The `KFold` library is designed to split the data into training and testing subsets. It does this by offering an iterable object that can be initialised with\n",
    "\n",
    "```python\n",
    "kf = KFold(n_splits=k, shuffle=True)\n",
    "```\n",
    "\n",
    "The `n_splits=k` specifies the number of subsets to use.\n",
    "\n",
    "By default `shuffle` is set to `False`.\n",
    "It is generally good practice to shuffle the data for cross validation as sometimes during collection and storage, data of a similar type can be stored adjacently which would lead to some learning bias when training the tree.\n",
    "For example, if the data was sorted by redshift, on the first iteration the model might be trained with redshifts 0 to 3 and tested on galaxies with redshifts ~4.\n",
    "\n",
    "In the next couple of problems we will use the `sklearn` library `KFold` to help us split our data into our $k-1$ training subsets and remaining test subset.\n",
    "In the first problem we will use the convenience of `KFolds` to help us calculate the $k$-fold cross validated accuracy of our model. In the second we will extend this to provide a $k$-folded cross validated prediction for every galaxy in our data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: KFold cross validation\n",
    "\n",
    "Your task is to complete the function `cross_validate_model`.\n",
    "The function takes 4 arguments:\n",
    "\n",
    "- `model`, `feaures`, and `targets` as in previous problems;\n",
    "- `k` in our $k$-fold.\n",
    "This is the number of subsets to train and test.\n",
    "\n",
    "Your function should return a list containing the `k` median of differences for each of the `k` folds using `median_diff`.\n",
    "\n",
    "Note that we have set the `max_depth=19` when we initialise the decision tree to prevent the model from overfitting.\n",
    "\n",
    "**KFolds usage**\n",
    "\n",
    "We have created the KFold object to give you a set of training and testing indices for each of the `k` runs.\n",
    "It is worth taking a moment to understand this.\n",
    "\n",
    "Specifically, the object is initialised with\n",
    "\n",
    "```python\n",
    "kf = KFold(n_splits=k, shuffle=True)\n",
    "```\n",
    "\n",
    "The `n_splits=k` passes our desired number of subsets/folds.\n",
    "We want to shuffle the data (as previously explained).\n",
    "The iterator is then used with:\n",
    "\n",
    "```python\n",
    "for train_indices, test_indices in kf.split(features):\n",
    "```\n",
    "The `kf.split(features)` is an iterator that, for each of the `k` iterations, returns two arrays of indices to be used with our feature and target arrays, i.e. `features[train_indices],targets[train_indices]`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# complete this function\n",
    "def cross_validate_model(model, features, targets, k):\n",
    "  kf = KFold(n_splits=k, shuffle=True)\n",
    "\n",
    "  # initialise a list to collect median_diffs for each iteration of the loop below\n",
    "\n",
    "  for train_indices, test_indices in kf.split(features):\n",
    "    train_features, test_features = features[train_indices], features[test_indices]\n",
    "    train_targets, test_targets = targets[train_indices], targets[test_indices]\n",
    "\n",
    "    # fit the model for the current set\n",
    "\n",
    "    # predict using the model\n",
    "\n",
    "    # calculate the median_diff from predicted values and append to results array\n",
    "\n",
    "\n",
    "  # return the list with your median difference values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# complete this function\n",
    "def cross_validate_model(model, features, targets, k):\n",
    "  kf = KFold(n_splits=k, shuffle=True)\n",
    "\n",
    "  # initialise a list to collect median_diffs for each iteration of the loop below\n",
    "  diffs = []\n",
    "\n",
    "  for train_indices, test_indices in kf.split(features):\n",
    "    train_features, test_features = features[train_indices], features[test_indices]\n",
    "    train_targets, test_targets = targets[train_indices], targets[test_indices]\n",
    "\n",
    "    # fit the model for the current set\n",
    "    model.fit(train_features, train_targets)\n",
    "\n",
    "    # predict using the model\n",
    "    predictions = model.predict(test_features)\n",
    "\n",
    "    # calculate the median_diff from predicted values and append to results array\n",
    "    diffs.append(median_diff(predictions, test_targets))\n",
    "\n",
    "  # return the list with your median difference values\n",
    "  return diffs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::::::{dropdown} Click here for explanation\n",
    "\n",
    "This is quite similar to our earlier implementation for validation in the last tutorial.\n",
    "The main differences are that we are using the `train_indices` and `test_indices` to split our features and target arrays.\n",
    "\n",
    "Another difference is that we are collecting our calculated med_diff values in a list to be returned at the end of the function.\n",
    "\n",
    "::::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.load('data/sdss_galaxy_colors.npy')\n",
    "features, targets = get_features_targets(data)\n",
    "\n",
    "# initialize model with a maximum depth of 19\n",
    "dtr = DecisionTreeRegressor(max_depth=19)\n",
    "\n",
    "# call your cross validation function\n",
    "diffs = cross_validate_model(dtr, features, targets, 10)\n",
    "\n",
    "# Print the values\n",
    "print('Differences: {}'.format(', '.join(['{:.3f}'.format(val) for val in diffs])))\n",
    "print('Mean difference: {:.3f}'.format(np.mean(diffs)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross validation of predictions\n",
    "\n",
    "Cross validation is an important part of ensuring that our model is returning values that are at least partially accurate.\n",
    "The problem with held-out validation is that the we are only able to get prediction values for the data in our test set.\n",
    "\n",
    "With $k$-fold cross validation each galaxy is tested at least once and because of this we are able to get a prediction value for every galaxy.\n",
    "We'll do this in the next question."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: KFold Cross Validated Predictions\n",
    "\n",
    "Complete the function `cross_validate_predictions`.\n",
    "This is very similar to the previous question except instead of returning the `med_diff` accuracy measurements we would like to return a predicted value for each of the galaxies.\n",
    "\n",
    "The function takes the same 4 arguments as the previous question, i.e. `model`, `feaures`, `targets` and `k`.\n",
    "\n",
    "Your function should return a single variable.\n",
    "The returned variable should be a 1-D numpy array of length\n",
    "$m$, where $m$ is the number of galaxies in our data set.\n",
    "You should make sure that you maintain the order of galaxies when giving your predictions, such that the first prediction in your array corresponds to the first galaxy in the `features` and `targets` arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# complete this function\n",
    "def cross_validate_predictions(model, features, targets, k):\n",
    "  kf = KFold(n_splits=k, shuffle=True)\n",
    "\n",
    "  # declare an array for predicted redshifts from each iteration\n",
    "  all_predictions = np.zeros_like(targets)\n",
    "\n",
    "  for train_indices, test_indices in kf.split(features):\n",
    "    # split the data into training and testing\n",
    "\n",
    "    # fit the model for the current set\n",
    "\n",
    "    # predict using the model\n",
    "\n",
    "    # put the predicted values in the all_predictions array defined above\n",
    "    all_predictions[test_indices] = predictions\n",
    "\n",
    "  # return the predictions\n",
    "  return all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# complete this function\n",
    "def cross_validate_predictions(model, features, targets, k):\n",
    "  kf = KFold(n_splits=k, shuffle=True)\n",
    "\n",
    "  # declare an array for predicted redshifts from each iteration\n",
    "  all_predictions = np.zeros_like(targets)\n",
    "\n",
    "  for train_indices, test_indices in kf.split(features):\n",
    "    # split the data into training and testing\n",
    "    train_features, test_features = features[train_indices], features[test_indices]\n",
    "    train_targets, test_targets = targets[train_indices], targets[test_indices]\n",
    "\n",
    "    # fit the model for the current set\n",
    "    model.fit(train_features, train_targets)\n",
    "\n",
    "    # predict using the model\n",
    "    predictions = model.predict(test_features)\n",
    "\n",
    "    # put the predicted values in the all_predictions array defined above\n",
    "    all_predictions[test_indices] = predictions\n",
    "\n",
    "  # return the predictions\n",
    "  return all_predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::::::{dropdown} Click here for explanation\n",
    "This is very similar to the previous problem.\n",
    "Here instead of using the predicted values for calculating the accuracy we simply put them into an array which the function returns.\n",
    "We initialise an array to store the predictions from each validation with...\n",
    "\n",
    "```python\n",
    "all_predictions = np.zeros(shape = (len(targets)))\n",
    "```\n",
    "\n",
    "We then use the test_indices to keep the correct order when populating the array.\n",
    "\n",
    "```python\n",
    "all_predictions[test_indices] = predicted\n",
    "```\n",
    "This ensures that we can compare the predictions their corresponding target values later when calculating the median difference and plotting the predicted values against actual values.\n",
    "::::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'data/sdss_galaxy_colors.npy'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/arunkannawadi/Desktop/code/data-driven-astronomy/data_driven_astronomy/notebooks/5b.ipynb Cell 19\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/arunkannawadi/Desktop/code/data-driven-astronomy/data_driven_astronomy/notebooks/5b.ipynb#ch0000020?line=0'>1</a>\u001b[0m data \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mdata/sdss_galaxy_colors.npy\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/arunkannawadi/Desktop/code/data-driven-astronomy/data_driven_astronomy/notebooks/5b.ipynb#ch0000020?line=1'>2</a>\u001b[0m features, targets \u001b[39m=\u001b[39m get_features_targets(data)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/arunkannawadi/Desktop/code/data-driven-astronomy/data_driven_astronomy/notebooks/5b.ipynb#ch0000020?line=3'>4</a>\u001b[0m \u001b[39m# initialize model\u001b[39;00m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/numpy/lib/npyio.py:407\u001b[0m, in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    405\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 407\u001b[0m     fid \u001b[39m=\u001b[39m stack\u001b[39m.\u001b[39menter_context(\u001b[39mopen\u001b[39;49m(os_fspath(file), \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m))\n\u001b[1;32m    408\u001b[0m     own_fid \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    410\u001b[0m \u001b[39m# Code to distinguish from NumPy binary files and pickles.\u001b[39;00m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'data/sdss_galaxy_colors.npy'"
     ]
    }
   ],
   "source": [
    "data = np.load('data/sdss_galaxy_colors.npy')\n",
    "features, targets = get_features_targets(data)\n",
    "\n",
    "# initialize model\n",
    "dtr = DecisionTreeRegressor(max_depth=19)\n",
    "\n",
    "# call your cross validation function\n",
    "predictions = cross_validate_predictions(dtr, features, targets, 10)\n",
    "\n",
    "# calculate and print the rmsd as a sanity check\n",
    "diffs = median_diff(predictions, targets)\n",
    "print('Median difference: {:.3f}'.format(diffs))\n",
    "\n",
    "# plot the results to see how well our model looks\n",
    "plt.scatter(targets, predictions, s=0.4)\n",
    "plt.xlim((0, targets.max()))\n",
    "plt.ylim((0, predictions.max()))\n",
    "plt.xlabel('Measured Redshift')\n",
    "plt.ylabel('Predicted Redshift')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Fold discussion\n",
    "\n",
    "K-Fold cross validation is an important part of assessing the accuracy of any machine learning model.\n",
    "When we plotted our predicted vs measured redshifts we are able to see that for many our galaxies we were able to get a reasonably accurate prediction of redshift.\n",
    "However, there are also several outliers where our model does not give a good prediction.\n",
    "\n",
    "![Scatter plot of predicted vs. measured redshift](https://groklearning-cdn.com/modules/SjroKib6Hs5Fqxq53Vxme9/predicted_v_measured.png)\n",
    "\n",
    "We have learnt the inner workings of $k$-Fold cross validation with the help of the `KFold` library.\n",
    "Now that you have a working understanding of $k$-Fold you should be aware that there are several methods and libraries in the `sklearn.model_selection` modules that provide off the shelf versions of some of the routines that we have just written.\n",
    "\n",
    "The `cross_val_predict` function performs the same actions as the `cross_validate_predictions` function you wrote in the previous question.\n",
    "It can be called with\n",
    "```python\n",
    "predictions = cross_val_predict(dtr, features, targets, cv=k)\n",
    "```\n",
    "where `dtr` is our decision tree regressor object, `cv=k` allows us to specify the number of folds(k) to use and `features`/`targets` are as we have used them so far.\n",
    "\n",
    "There is one other tool in the `sklearn.model_selection` library that is worth noting, the `cross_val_score` function.\n",
    "This provides a score of how well the model performed similar to the `med_diff` we have been using so far.\n",
    "We will not go into the usage here, but you need to specify which metric is used to score the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QSO vs Galaxies\n",
    "\n",
    "You might be surprised to learn that our sample of galaxies consists of two different populations: regular galaxies and quasi-stellar objects (QSOs).\n",
    "QSOs are a type of galaxy that contain an actively (and intensely) accreting supermassive black hole.\n",
    "This is often referred to as an Active Galactic Nucleus (AGN).\n",
    "\n",
    "![AGN](https://groklearning-cdn.com/modules/B9yuTkmwbvVYh65FcYjb2f/agn.png)\n",
    "\n",
    "The light emitted from the AGN is significantly brighter than the rest of the galaxy and we are able to detect these QSOs out to much higher redshifts.\n",
    "In fact, most of the normal galaxies we have been using to create our models have redshifts less than $z \\approx 0.4$, while the QSOs have redshifts all the way out to $z \\approx 6$.\n",
    "Due to this contribution from the AGN, the flux magnitudes measured at different wavelengths might not follow the typical profile we assumed when predicting redshifts.\n",
    "\n",
    "In the next question we are going look at whether there is a difference in the accuracy of the decision trees between QSOs and regular galaxies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise: QSO and Galaxy\n",
    "\n",
    "Write a function `split_galaxies_qsos` that splits our data containing both galaxies and QSOs into two arrays that contain only galaxies and QSOs respectively.\n",
    "Your function should take a single `data` argument.\n",
    "\n",
    "The function should return two NumPy arrays, the first `galaxies` containing only rows from `data` that are galaxies and the second `qsos` containing only rows that are QSOs.\n",
    "\n",
    "The data array contains a column `data['spec_class']` where the values will either be `b'GALAXY'` or `b'QSO'`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::::::{admonition} Gotcha: use b'GALAXY' and not 'GALAXY'\n",
    "The spectral class is stored as a byte string (not Unicode strings), so the literals must have a b out the front. Comparing against 'GALAXY' will not match any rows, whereas b'GALAXY' will.\n",
    "::::::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::::::{admonition} Hint: use masking to select the rows\n",
    ":class: tip\n",
    "We can use masking to select particular rows:\n",
    "```python\n",
    "import numpy as np\n",
    "data = np.load('sdss_galaxy_colors.npy')\n",
    "galaxies = data[data['spec_class'] == b'GALAXY']\n",
    "```\n",
    "The inner `data['spec_class'] == b'GALAXY'` returns all of the indices that have a galaxy spectral type. These indices are then used to select the rows with the outer `data[...]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# complete this function\n",
    "def split_galaxies_qsos(data):\n",
    "  pass\n",
    "  # split the data into galaxies and qsos arrays\n",
    "\n",
    "  # return the seperated galaxies and qsos arrays\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Solution\n",
    "def split_galaxies_qsos(data):\n",
    "  # split the data into galaxies and qsos arrays\n",
    "  galaxies = data[data['spec_class'] == b'GALAXY']\n",
    "  qsos = data[data['spec_class'] == b'QSO']\n",
    "\n",
    "  # return the seperated galaxies and qsos arrays\n",
    "  return galaxies, qsos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::::::{dropdown} Click here for explanation\n",
    "\n",
    "This solution uses masking.\n",
    "```python\n",
    "qso_mask = data['spec_class'] == b'QSO'\n",
    "```\n",
    "Creates an array of boolean values `True` if the `data['spec_class'] == b'QSO'` and `False` otherwise.\n",
    "\n",
    "We do the same for the galaxies and then create our two return arrays using the masks.\n",
    "\n",
    "::::::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_median_diff(data):\n",
    "  features, targets = get_features_targets(data)\n",
    "  dtr = DecisionTreeRegressor(max_depth=19)\n",
    "  return np.mean(cross_validate_model(dtr, features, targets, 10))\n",
    "\n",
    "data = np.load('data/sdss_galaxy_colors.npy')\n",
    "\n",
    "# Split the data set into galaxies and QSOs\n",
    "galaxies, qsos= split_galaxies_qsos(data)\n",
    "\n",
    "# Here we cross validate the model and get the cross-validated median difference\n",
    "# The cross_validated_med_diff function is in \"written_functions\"\n",
    "galaxy_med_diff = cross_validate_median_diff(galaxies)\n",
    "qso_med_diff = cross_validate_median_diff(qsos)\n",
    "\n",
    "# Print the results\n",
    "print(\"Median difference for Galaxies: {:.3f}\".format(galaxy_med_diff))\n",
    "print(\"Median difference for QSOs: {:.3f}\".format(qso_med_diff))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## QSO discussion\n",
    "\n",
    "So our QSOs have a greater median residual $(\\approx 0.074)$ than the galaxies $(\\approx 0.016)$.\n",
    "There are a couple of possibilities why this is the case.\n",
    "\n",
    "There are far fewer QSOs (8525) than galaxies (41,475).\n",
    "Galaxies aren't as bright as QSOs so they become too faint to be detected with SDSS at redshifts $\\approx 0.4$.\n",
    "This creates a measurement bias.\n",
    "When I take a random sample of galaxies the same size as the QSO data set I get a `med_diff` of $\\approx 0.018$, which is slightly higher than the full set, but not enough to account for the gap between the two populations.\n",
    "\n",
    "The figure below shows the normalised distribution function of the two populations.\n",
    "\n",
    "![Normalized redshift distribution of QSOs and galaxies](https://groklearning-cdn.com/modules/g9BapNhtoxZihnccvW29Ug/redshift_distribution.png)\n",
    "\n",
    "We can see that the majority of galaxies form a peak around $0.10$ while the QSOs are resonably evenly distributed out to redshift $\\approx 2.5$.\n",
    "This can lead to a measurement bias.\n",
    "In the case of the galaxies we have trained our decision tree with target redshifts approximately less than $0.4$.\n",
    "As such the predictions from this model will not be larger than the maximum target value.\n",
    "So the maximum difference (or residual) for each galaxy in this set will be a lot smaller than the maximum residual for the QSOs.\n",
    "\n",
    "We can often get a clearer view of this by looking at the predicted redshifts vs actual redshifts in a plot.\n",
    "\n",
    "![Scatter plot of predicted vs. measured redshift, colored by QSOs and galaxies](https://groklearning-cdn.com/modules/ovFSymwFkqBPAcjnbSUxLG/predicted_actual_qso.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "We have looked at how decision trees are prone to overfitting the model and how limiting the maximum depth of the tree can be used to prevent this. By comparing the accuracy of the model on the training set with that of the test set for different tree depths we found that a maximum tree depth of 19 was suitable for our model.\n",
    "\n",
    "We looked at $k$-fold cross validation and the various methods that can be used to implement it.\n",
    "$k$-fold cross validation mitigates the risk that the training set has a unique or specific population of the data set; For example if all the training data contained QSOs and the testing set regular galaxies.\n",
    "$k$-folds cross validation also allows you to get a prediction for all the points in your data set.\n",
    "\n",
    "We concluded by looking at the sub-population of QSOs and how their accuracy measurement was significantly worse than that of the other galaxies. On closer inspection we found that this was a measurement bias resulting from the difference in the range of redshifts in each population.\n",
    "\n",
    "You have hopefully learnt all the tools necessary to implement a decision tree on a regression problem of your own. In the next module we will look at how decision trees can be used for classification. We will be using them to classify galaxies as either an elliptical, a spiral or a merger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "data-driven-astronomy",
   "language": "python",
   "name": "data-driven-astronomy"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
